{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 14:38:26.111883: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-12 14:38:26.114536: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-12 14:38:26.167826: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-12 14:38:26.169507: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 14:38:27.148976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import dlib\n",
    "import cvlib as cv\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dir,saved):\n",
    "    categories = [\"down\", \"front\", \"left\", \"right\", \"up\"]\n",
    "    flat_data_arr = []\n",
    "    target_arr = []\n",
    "    datadir =dir# \"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\Dataset_for_student_registration\"\n",
    "    for i in categories:\n",
    "        path = os.path.join(datadir, i)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path, img))\n",
    "            h, w, _ = img_array.shape\n",
    "            resized_img = cv2.resize(img_array, (200, 200))\n",
    "            fd, _ = hog(resized_img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                cells_per_block=(1, 1), visualize=True, channel_axis=-1)\n",
    "            flat_data_arr.append(fd)\n",
    "            target_arr.append(categories.index(i))\n",
    "    flat_data = np.array(flat_data_arr)\n",
    "    target = np.array(target_arr)\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    df['Target'] = target\n",
    "    x = df.iloc[:, :-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "    svc=svm.SVC(probability=True)\n",
    "    model=GridSearchCV(svc,param_grid)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, stratify = y)\n",
    "    try:\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        filepath = saved#\"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\model_new_no_normalization.sav\"\n",
    "        pickle.dump(model, open(filepath, \"wb\"))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6     \\\n",
      "0    0.427585  0.427585  0.427585  0.051007  0.427585  0.288538  0.427585   \n",
      "1    0.406238  0.406238  0.406238  0.406238  0.406238  0.272589  0.249614   \n",
      "2    0.541065  0.541065  0.541065  0.115187  0.270749  0.071725  0.045623   \n",
      "3    0.258766  0.146506  0.462053  0.462053  0.462053  0.462053  0.232518   \n",
      "4    0.681456  0.182604  0.119314  0.040051  0.068981  0.043858  0.123979   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "371  0.478899  0.478899  0.478899  0.227336  0.298524  0.120980  0.390340   \n",
      "372  0.501662  0.570884  0.570884  0.109747  0.207726  0.100328  0.143907   \n",
      "373  0.612847  0.612847  0.417424  0.027789  0.114240  0.019650  0.125140   \n",
      "374  0.377664  0.405046  0.366556  0.429222  0.449346  0.378772  0.151097   \n",
      "375  0.423703  0.423703  0.310017  0.257795  0.423703  0.145011  0.423703   \n",
      "\n",
      "         7         8         9     ...      1142      1143      1144  \\\n",
      "0    0.000000  0.444535  0.094889  ...  0.400225  0.400225  0.439252   \n",
      "1    0.195558  0.414545  0.122503  ...  0.382229  0.382229  0.530759   \n",
      "2    0.167171  0.456332  0.255153  ...  0.353553  0.353553  0.473344   \n",
      "3    0.059498  0.390262  0.038127  ...  0.206448  0.242997  0.419438   \n",
      "4    0.681456  0.388678  0.257207  ...  0.347121  0.528595  0.371304   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "371  0.064567  0.443768  0.511228  ...  0.416466  0.068716  0.392401   \n",
      "372  0.102709  0.524518  0.554002  ...  0.009766  0.000000  0.453363   \n",
      "373  0.211480  0.547818  0.547818  ...  0.469863  0.469863  0.492662   \n",
      "374  0.080641  0.637921  0.637921  ...  0.404089  0.102936  0.462400   \n",
      "375  0.313536  0.535444  0.535444  ...  0.371924  0.371924  0.358412   \n",
      "\n",
      "         1145      1146      1147      1148      1149      1150      1151  \n",
      "0    0.172720  0.439252  0.073880  0.439252  0.439252  0.439252  0.000000  \n",
      "1    0.530759  0.353086  0.199363  0.166776  0.096913  0.379999  0.300946  \n",
      "2    0.473344  0.473344  0.353279  0.291185  0.084576  0.190508  0.273490  \n",
      "3    0.419438  0.419438  0.419438  0.419438  0.207500  0.139804  0.240323  \n",
      "4    0.371304  0.371304  0.186898  0.371304  0.371304  0.371304  0.371304  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "371  0.392401  0.392401  0.392401  0.392401  0.076355  0.339332  0.330349  \n",
      "372  0.453363  0.453363  0.453363  0.383671  0.047434  0.117011  0.121249  \n",
      "373  0.492662  0.312813  0.054096  0.122523  0.138183  0.492662  0.370091  \n",
      "374  0.462400  0.462400  0.000000  0.335562  0.088275  0.462400  0.156050  \n",
      "375  0.358412  0.358412  0.358412  0.358412  0.358412  0.358412  0.317467  \n",
      "\n",
      "[376 rows x 1152 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "371    4\n",
      "372    4\n",
      "373    4\n",
      "374    4\n",
      "375    4\n",
      "Name: Target, Length: 376, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "categories = [\"down\", \"front\", \"left\", \"right\", \"up\"]\n",
    "flat_data_arr = []\n",
    "target_arr = []\n",
    "datadir = \"/home/spanidea/Documents/aicv_proctor1/aicv_proctor/dataset/Dataset_for_student_registration\"\n",
    "\n",
    "for i in categories:\n",
    "    path = os.path.join(datadir, i)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img))\n",
    "        h, w, _ = img_array.shape\n",
    "        resized_img = cv2.resize(img_array, (200, 200))\n",
    "        fd, _ = hog(resized_img, orientations=8, pixels_per_cell=(16, 16),\n",
    "            cells_per_block=(1, 1), visualize=True, channel_axis=-1)\n",
    "        flat_data_arr.append(fd)\n",
    "        target_arr.append(categories.index(i))\n",
    "        \n",
    "flat_data = np.array(flat_data_arr)\n",
    "target = np.array(target_arr)\n",
    "df = pd.DataFrame(flat_data)\n",
    "df['Target'] = target\n",
    "x = df.iloc[:, :-1]\n",
    "y = df.iloc[:,-1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 1152)\n",
      "(376,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "svc=svm.SVC(probability=True)\n",
    "model=GridSearchCV(svc,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, stratify = y)\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1152)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100],\n",
       "                         &#x27;gamma&#x27;: [0.0001, 0.001, 0.1, 1],\n",
       "                         &#x27;kernel&#x27;: [&#x27;rbf&#x27;, &#x27;poly&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100],\n",
       "                         &#x27;gamma&#x27;: [0.0001, 0.001, 0.1, 1],\n",
       "                         &#x27;kernel&#x27;: [&#x27;rbf&#x27;, &#x27;poly&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=SVC(probability=True),\n",
       "             param_grid={'C': [0.1, 1, 10, 100],\n",
       "                         'gamma': [0.0001, 0.001, 0.1, 1],\n",
       "                         'kernel': ['rbf', 'poly']})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 98.68421052631578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"Accuracy score\", accuracy_score(y_pred, y_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_arr = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    correct = correct + 1 if y_pred[i] == y_test_arr[i] else correct\n",
    "print(correct)\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\model_new_no_normalization.sav\"\n",
    "pickle.dump(model, open(filepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\model_new_no_normalization.sav\"\n",
    "loaded_model = pickle.load(open(filepath, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_path = \"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\scaler.obj\"\n",
    "pickle.dump(scaler, open(scaler_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_path = \"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\scaler.obj\"\n",
    "scaler_file = pickle.load(open(scaler_path, \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE SURE TO USE cv2.imread OUTSIDE THE FUNCTION AS THE FUNCTION TAKES IMAGE ARRAY AS AN ARGUMENT AND NOT THE PATH TO THE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler_path = \"C:\\\\Users\\\\nites\\\\OneDrive\\\\Desktop\\\\proctoring_software\\\\scaler.obj\"\n",
    "#scaler_file = pickle.load(open(scaler_path, \"rb\"))\n",
    "categories = [\"down\", \"front\", \"left\", \"right\", \"up\"]\n",
    "\n",
    "def predict_label(model, img_array):\n",
    "    \n",
    "    img_resized = cv2.resize(img_array, (200, 200))\n",
    "\n",
    "    fd, _ = hog(img_resized, orientations=8, pixels_per_cell=(16, 16),\n",
    "                cells_per_block=(1, 1), visualize=True, channel_axis=-1)\n",
    "    \n",
    "    #data_arr = scaler_file.transform([fd])\n",
    "    probability = model.predict_proba([fd])\n",
    "    predicted_class = model.predict([fd])\n",
    "    predicted_class = predicted_class[0]\n",
    "    return categories[predicted_class], probability[0][predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('down', 0.9346094303562799)\n",
      "INFERENCE TIME:  0.04828023910522461\n"
     ]
    }
   ],
   "source": [
    "initial_time = time.time()\n",
    "img = cv2.imread(\"Dataset_for_student_registration\\\\down\\\\img_5.jpg\")\n",
    "print(predict_label(loaded_model, img))\n",
    "final_time = time.time()\n",
    "print(\"INFERENCE TIME: \", final_time - initial_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[146, 145, 336, 399]]\n",
      "[[145, 145, 336, 401]]\n",
      "[[146, 145, 336, 402]]\n",
      "[[146, 146, 336, 402]]\n",
      "[[147, 147, 336, 401]]\n",
      "[[146, 148, 336, 403]]\n",
      "[[145, 148, 336, 404]]\n",
      "[[145, 147, 336, 402]]\n",
      "[[144, 147, 336, 404]]\n",
      "[[143, 147, 335, 405]]\n",
      "[[143, 148, 335, 403]]\n",
      "[[143, 148, 335, 403]]\n",
      "[[143, 147, 336, 404]]\n",
      "[[143, 148, 336, 405]]\n",
      "[[143, 148, 336, 405]]\n",
      "[[143, 149, 336, 405]]\n",
      "[[143, 149, 336, 406]]\n",
      "[[143, 149, 336, 406]]\n",
      "[[143, 148, 336, 406]]\n",
      "[[143, 148, 337, 406]]\n",
      "[[143, 148, 337, 405]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[142, 149, 337, 407]]\n",
      "[[142, 148, 337, 407]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[143, 148, 336, 407]]\n",
      "[[143, 148, 336, 407]]\n",
      "[[143, 147, 337, 407]]\n",
      "[[143, 147, 336, 407]]\n",
      "[[143, 148, 336, 407]]\n",
      "[[143, 148, 336, 407]]\n",
      "[[142, 148, 336, 407]]\n",
      "[[142, 148, 336, 407]]\n",
      "[[142, 150, 335, 407]]\n",
      "[[142, 150, 335, 407]]\n",
      "[[142, 149, 335, 407]]\n",
      "[[142, 149, 336, 407]]\n",
      "[[143, 148, 337, 407]]\n",
      "[[143, 149, 337, 406]]\n",
      "[[148, 153, 338, 404]]\n",
      "[[149, 151, 339, 404]]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "\n",
    "    face_locations, _ = cv.detect_face(frame, threshold = 0.3)\n",
    "#cropped_image = frame\n",
    "    print(face_locations)\n",
    "    left, top, right, bottom = face_locations[0]\n",
    "    cropped_image = frame[top:bottom, left:right]\n",
    "#cropped_image = cv2.resize(cropped_image, (400, 400))\n",
    "    prediction_label, prediction_probability = predict_label(loaded_model, cropped_image)\n",
    "    if face_locations != None:\n",
    "        cv2.putText(frame, str(face_locations), (100, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (150, 200, 30), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"FACE NOT DETECTED\", (200, 500), cv2.FONT_HERSHEY_PLAIN, 2, (150, 200, 30), 2)\n",
    "    #print(prediction_probability)\n",
    "    #cv2.putText(frame, prediction_label, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 200, 200), 3)\n",
    "    #cv2.putText(frame, str(prediction_probability), (100, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0, 0), 3)\n",
    "    \n",
    "\n",
    "        #cv2.putText(frame, \"FACE NOT DETECTED\", (200, 500), cv2.FONT_HERSHEY_PLAIN, 2, (150, 200, 30), 2)\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "56412660b037274d686266c65699192c47fe0cde2b795cf57673260ecc54f434"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
